import re
import random
import html
import pylcs
import numpy as np

def format_reward(completion):
    """Reward function that checks if the reasoning process is enclosed within <think> and </think> tags,
    while the final answer is enclosed within <answer> and </answer> tags."""
    pattern1 = r"^<think>\n.*?\n</think>\n<answer>\n.*?\n</answer>$"
    match1 = re.match(pattern1, completion, re.DOTALL | re.MULTILINE)

    pattern2 = r"^<think>\n.*?\n</think>\n<answer>\s.*?\s</answer>$"
    match2 = re.match(pattern2, completion, re.DOTALL | re.MULTILINE)

    pattern3 = r"^<think>\s.*?\s</think>\n<answer>\n.*?\n</answer>$"
    match3 = re.match(pattern3, completion, re.DOTALL | re.MULTILINE)

    pattern4 = r"^<think>\s.*?\s</think>\n<answer>\s.*?\s</answer>$"
    match4 = re.match(pattern4, completion, re.DOTALL | re.MULTILINE)
    if match1 or match2 or match3 or match4:
        return 0.0
    else:
        return -1.0


def tag_count_reward(completion):
   """Reward function that checks if we produce the desired number of think and answer tags associated with `format_reward()`."""
   count = 0.0

   # Check for opening think tags (using regex to capture both formats)
   think_start_pattern = r"<think>\n|<think>\s"
   think_start_matches = re.findall(think_start_pattern, completion)
   if len(think_start_matches) != 1:
       count -= 0.25

   # Check for closing think tags
   think_end_pattern = r"\n</think>\n|\s</think>\n"
   think_end_matches = re.findall(think_end_pattern, completion)
   if len(think_end_matches) != 1:
       count -= 0.25

   # Check for opening answer tags
   answer_start_pattern = r"\n<answer>\n|\n<answer>\s"
   answer_start_matches = re.findall(answer_start_pattern, completion)
   if len(answer_start_matches) != 1:
       count -= 0.25

   # Check for closing answer tags
   answer_end_pattern = r"\n</answer>|\s</answer>"
   answer_end_matches = re.findall(answer_end_pattern, completion)
   if len(answer_end_matches) != 1:
       count -= 0.25

   return count

def match_and_order_lists(generated_list, candidate_list):
    """
    Matches items from a generated list to a candidate list and returns an ordered list
    of matched candidate items, ensuring no repeated matches.

    Args:
        generated_list: A list of strings generated by a model or process.
        candidate_list: A list of strings representing candidate items to match against.

    Returns:
        A list of strings from the candidate_list that were matched to items in the
        generated_list, maintaining the order of the first match in the generated list
        and ensuring no candidate item is matched more than once.
    """
    matched_candidate_items = []
    used_candidate_indices = set()

    for generated_item_detail in generated_list:
        if not generated_item_detail:
            continue

        # Clean up the generated item text
        pr = generated_item_detail.find('. ')
        if generated_item_detail[:pr].isdigit():
            generated_item_name = generated_item_detail[pr + 2:].strip()
        else:
            generated_item_name = generated_item_detail.strip()

        matched_name = None
        matched_candidate_index = -1

        for i, candidate_text_single in enumerate(candidate_list):
            clean_candidate_text_single = html.unescape(candidate_text_single.strip())

            # Define your matching criteria here. You can adjust these conditions.
            if (clean_candidate_text_single in generated_item_name) or \
               (generated_item_name in clean_candidate_text_single) or \
               (pylcs.lcs_sequence_length(generated_item_name, clean_candidate_text_single) > 0.9 * len(clean_candidate_text_single)):
                if i not in used_candidate_indices:
                    matched_name = candidate_text_single
                    matched_candidate_index = i
                    break

        if matched_name is not None:
            matched_candidate_items.append(matched_name)
            used_candidate_indices.add(matched_candidate_index)

    return matched_candidate_items


def extract_solution(solution_str):
    """
    Combined extraction function that handles both strict/flexible formats and multiple answer tags.

    Processing:
    1. Try strict format matching first (requires exact newlines)
    2. Fall back to flexible format matching (allows any whitespace)
    3. If multiple matches are found, return the last one

    Args:
        solution_str: String containing the answer(s)

    Returns:
        Extracted answer, or None if no valid format matches
    """
    # Check for multiple matches (from second function)
    answer_pattern = r'<answer>\s*(.*?)\s*</answer>'
    matches = list(re.finditer(answer_pattern, solution_str, re.DOTALL))

    # If there are 2 or more matches, return the last one
    if len(matches) >= 2:
        return matches[-1].group(1).strip()

    # Try strict format matching (from first function)
    strict_pattern = r"<answer>\n(.*?)\n</answer>"
    strict_match = re.search(strict_pattern, solution_str, re.DOTALL)

    if strict_match:
        return strict_match.group(1)

    # If strict matching fails, try flexible format matching
    flexible_pattern = r"<answer>\s*(.*?)\s*</answer>"
    flexible_match = re.search(flexible_pattern, solution_str, re.DOTALL)

    if flexible_match:
        return flexible_match.group(1).strip()

    # If no format matches, return None
    return None






def calculate_f1_score(ground_truth_list, predicted_list):
    """
    Calculate F1 score between ground truth and predicted lists of strings.

    Args:
        ground_truth_list: List of strings representing the ground truth items
        predicted_list: List of strings representing the predicted/recommended items

    Returns:
        f1_score: The F1 score (harmonic mean of precision and recall)
    """
    # Convert lists to sets for easier intersection calculation
    ground_truth_set = set(ground_truth_list)
    predicted_set = set(predicted_list)

    # Calculate the number of true positives (correctly predicted items)
    true_positives = len(ground_truth_set.intersection(predicted_set))

    # Calculate precision: TP / (TP + FP)
    # Precision is the fraction of predicted items that are relevant
    precision = true_positives / len(predicted_set) if len(predicted_set) > 0 else 0

    # Calculate recall: TP / (TP + FN)
    # Recall is the fraction of relevant items that are predicted
    recall = true_positives / len(ground_truth_set) if len(ground_truth_set) > 0 else 0

    # Calculate F1 score: 2 * (precision * recall) / (precision + recall)
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return f1_score

def calculate_ndcg(matched_items, ground_truth_item, top_k=20):
   """
   Calculate NDCG@k (Normalized Discounted Cumulative Gain at k) for the case of
   a single positive example and multiple negative examples.

   Args:
       matched_items:  A list of strings containing matched candidate items.
                       Order matters as it represents the ranking of matches.
       ground_truth_item: A string representing the ground truth item (positive example).
       top_k:  An integer representing the cutoff position for NDCG (e.g., 20 for NDCG@20).

   Returns:
       A float representing the NDCG@k value.
       Returns 0.0 if matched_items is empty.
   """
   if not matched_items:
       return 0.0

   # 1. Calculate DCG@k
   dcg = 0.0
   for i, item in enumerate(matched_items[:top_k]):
       if item == ground_truth_item:
           relevance = 1
           dcg += relevance / np.log2(i + 2)

   # 2. Calculate IDCG@k (Ideal DCG@k)
   #    IDCG is always 1 / log2(2) = 1 in this case, since there's only one positive example
   #    and ideally it would be ranked first.
   idcg = 1.0 if ground_truth_item else 0.0  # Ensure ground_truth_item exists

   # 3. Normalize
   if idcg == 0.0:
       return 0.0
   else:
       ndcg = dcg / idcg
       return ndcg

def rec_reward(completions, ground_truth):
    response_list = completions.split('\n')[0]
    candidate_list = ground_truth['candidate_text'].tolist()
    final_list=match_and_order_lists(response_list, candidate_list)
    gt=ground_truth['gt']
    reward=calculate_ndcg(matched_items=final_list, ground_truth_item=gt, top_k=20)

    return reward




def format_rank_reward(completions, ground_truth):
    response_list = completions.split('\n')[0]
    candidate_list = ground_truth['candidate_text'].tolist()
    final_list = match_and_order_lists(response_list, candidate_list)

    final_set = set(final_list)
    candidate_set = set(candidate_list)

    common_elements = final_set.intersection(candidate_set)
    num_common = len(common_elements)

    num_candidate = len(candidate_set)

    if num_candidate == 0:
        # Handle the case where candidate_list is empty
        if len(final_set) == 0:
            return 0.0  # Both are empty, perfect match (penalty 0)
        else:
            return -1.0  # Final list is not empty, no match (penalty -1)
    # Calculate the Jaccard index (similarity)
    jaccard_index = num_common / num_candidate
    # Rescale to the range [-1, 0]
    penalty = jaccard_index - 1
    return penalty



def compute_score(solution_str, ground_truth, method='strict', format_score=0.1, score=1.):
    """The scoring function for countdown task.

    Args:
        solution_str: the solution text
        ground_truth: dictionary containing target number and available numbers
        method: the method to extract the solution
        format_score: the score for correct format but wrong answer
        score: the score for the correct answer
    """

    equation = extract_solution(solution_str=solution_str)
    do_print = random.randint(1, 64) == 1

    if do_print:
        print(f"--------------------------------")
        print(f"Extracted equation: {equation}")
        print(f"Solution string: {solution_str}")

    format_r = format_reward(solution_str)
    tag_r = tag_count_reward(solution_str)


    if equation is None:
        print(f"No equation found")
        return -1+format_r+tag_r
    else:
        # return rec_reward(equation, ground_truth)
        return format_rank_reward(equation,ground_truth)+rec_reward(equation,ground_truth)+format_r+tag_r
        # return format_rank_reward(equation, ground_truth)