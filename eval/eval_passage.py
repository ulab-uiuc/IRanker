import json
import os
import numpy as np
# from rank_bm25 import BM25Okapi
import re
import string
from typing import List, Dict, Any
import torch
import gc
from tqdm import tqdm
from vllm import LLM, SamplingParams
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pylcs
from datasets import load_dataset

# Configuration
CONFIG = {
    'Passage-5': {
        'checkpoint_path': '',
        'prompt_template':"Here is a query: {query}.\n" \
                          "And here are the candidate passages:\n{passages_text}\n\n" \
                          "Please think step by step according to the content of each passage and how well it supports or relates to the query. " \
                          "Select the least likely passage from the candidate list. Only return the passage ID corresponding to the excluded passage (e.g., \"passage x\"). " \
                          "You MUST choose one passage from the candidate list. You can not generate content that is not in the given candidate list."
    },
    'Passage-7': {
        'checkpoint_path': '',
        'prompt_template':"Here is a query: {query}.\n" \
                          "And here are the candidate passages:\n{passages_text}\n\n" \
                          "Please think step by step according to the content of each passage and how well it supports or relates to the query. " \
                          "Select the least likely passage from the candidate list. Only return the passage ID corresponding to the excluded passage (e.g., \"passage x\"). " \
                          "You MUST choose one passage from the candidate list. You can not generate content that is not in the given candidate list."
    },
    'Passage-9': {
        'checkpoint_path': '',
        'prompt_template':"Here is a query: {query}.\n" \
                          "And here are the candidate passages:\n{passages_text}\n\n" \
                          "Please think step by step according to the content of each passage and how well it supports or relates to the query. " \
                          "Select the least likely passage from the candidate list. Only return the passage ID corresponding to the excluded passage (e.g., \"passage x\"). " \
                          "You MUST choose one passage from the candidate list. You can not generate content that is not in the given candidate list."
    }
}

def preprocess_text(text: str) -> List[str]:
    """Preprocess text by converting to lowercase, removing punctuation, and tokenizing."""
    text = text.lower()
    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)
    return [token for token in text.split() if token.strip()]

def extract_solution(solution_str: str) -> str:
    """Extract solution from model output."""
    answer_pattern = r'<answer>\s*(.*?)\s*</answer>'
    matches = list(re.finditer(answer_pattern, solution_str, re.DOTALL))
    
    if len(matches) >= 2:
        return matches[-1].group(1).strip()
    
    strict_pattern = r"<answer>\n(.*?)\n</answer>"
    strict_match = re.search(strict_pattern, solution_str, re.DOTALL)
    if strict_match:
        return strict_match.group(1)
    
    flexible_pattern = r"<answer>\s*(.*?)\s*</answer>"
    flexible_match = re.search(flexible_pattern, solution_str, re.DOTALL)
    if flexible_match:
        return flexible_match.group(1).strip()
    
    return None

def find_most_similar(response: str, candidate_texts: List[str]) -> str:
    """Find most similar text from candidates using TF-IDF and cosine similarity."""
    vectorizer = TfidfVectorizer()
    all_texts = [response] + candidate_texts
    tfidf_matrix = vectorizer.fit_transform(all_texts)
    similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()
    return candidate_texts[similarities.argmax()]

def match_and_order_lists(generated_list, candidate_list):
    """
    Matches items from a generated list to a candidate list and returns an ordered list
    of matched candidate items, ensuring no repeated matches.

    Args:
        generated_list: A list of strings generated by a model or process.
        candidate_list: A list of strings representing candidate items to match against.

    Returns:
        A list of strings from the candidate_list that were matched to items in the
        generated_list, maintaining the order of the first match in the generated list
        and ensuring no candidate item is matched more than once.
    """
    matched_candidate_items = []
    used_candidate_indices = set()

    for generated_item_detail in generated_list:
        if not generated_item_detail:
            continue

        # Clean up the generated item text
        pr = generated_item_detail.find('. ')
        if generated_item_detail[:pr].isdigit():
            generated_item_name = generated_item_detail[pr + 2:].strip()
        else:
            generated_item_name = generated_item_detail.strip()

        matched_name = None
        matched_candidate_index = -1

        for i, candidate_text_single in enumerate(candidate_list):
            clean_candidate_text_single = candidate_text_single.strip()

            # Define your matching criteria here. You can adjust these conditions.
            if (clean_candidate_text_single in generated_item_name) or \
               (generated_item_name in clean_candidate_text_single) or \
               (pylcs.lcs_sequence_length(generated_item_name, clean_candidate_text_single) > 0.9 * len(clean_candidate_text_single)):
                if i not in used_candidate_indices:
                    matched_name = candidate_text_single
                    matched_candidate_index = i
                    break

        if matched_name is not None:
            matched_candidate_items.append(matched_name)
            used_candidate_indices.add(matched_candidate_index)

    return matched_candidate_items

def make_prefix(query: str) -> str:
    """Create prefix for Qwen Instruct Models."""
    return f"""<|im_start|>system\nYou are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.<|im_end|>\n<|im_start|>user\n {query} Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags.<|im_end|>\n<|im_start|>assistant\nLet me solve this step by step.\n<think>"""

def construct_prompt(dataset_name: str, query: str, candidate_passages: Dict[str, str]) -> str:
    """Construct prompt for passage ranking."""
    if dataset_name not in CONFIG:
        raise ValueError(f"Unknown dataset: {dataset_name}. Available datasets: {list(CONFIG.keys())}")
    
    template = CONFIG[dataset_name]['prompt_template']
    passages_text = "\n".join([f"{pid}: {text}" for pid, text in candidate_passages.items()])
    return template.format(
        query=query,
        passages_text=passages_text
    )

def get_prompt(dataset_name: str, query: str, candidate_passages: Dict[str, str]) -> str:
    """Get complete prompt with prefix."""
    ini_query = construct_prompt(dataset_name, query, candidate_passages)
    return make_prefix(ini_query)

def process_batch(batch_data: List[Dict[str, Any]], llm: LLM, sampling_params: SamplingParams, dataset_name: str) -> List[Dict[str, Any]]:
    """Process a batch of samples in parallel until all reach final state."""
    # Initialize tracking for each sample in the batch
    active_samples = []
    for sample in batch_data:
        active_samples.append({
            'problem': sample['problem'],
            'gt': sample['gt'],
            'candidates': sample['candidates'].copy(),
            'original_length': len(sample['candidates']),
            'removed_passages': [],
            'ground_truth_rank': None,
            'is_complete': False,
            'reasoning': []  # Add list to store reasoning for each iteration
        })
    
    results = []
    iteration = 0
    rank_findings = {}  # Track number of ground truths found at each rank
    
    while True:
        iteration += 1
        print(f"\nIteration {iteration}:")
        print(f"{len(active_samples[0]['candidates'])}/{active_samples[0]['original_length']} passages remaining")
        
        # Prepare batch of prompts for active samples
        batch_prompts = []
        for sample in active_samples:
            user_his_text = sample['problem'].split("and candidate passages")[0].split("Here is a query:")[1].strip()
            prompt = get_prompt(dataset_name, user_his_text, sample['candidates'])
            batch_prompts.append(prompt)
        
        # Generate responses for all prompts in one batch
        outputs = llm.generate(batch_prompts, sampling_params)
        
        # Process results and update samples
        for output, sample in zip(outputs, active_samples):
            response = output.outputs[0].text.strip()
            
            # Extract reasoning (everything before </think>)
            reasoning = response.split("</think>")[0].strip() if "</think>" in response else ""
            sample['reasoning'].append(reasoning)
            
            passage_id = extract_solution(response)
            if passage_id is None:
                passage_id = response[-40:]
            
            # First check for exact match
            if passage_id in sample['candidates']:
                final_response = passage_id
            else:
                # Try to match using the more sophisticated matching function
                matched_items = match_and_order_lists([passage_id], list(sample['candidates'].keys()))
                if matched_items:
                    final_response = matched_items[0]
                else:
                    # Fall back to similarity-based matching
                    final_response = find_most_similar(passage_id, list(sample['candidates'].keys()))
            
            if passage_id != final_response:
                print(f"Original response: {passage_id}")
                print(f"Final matched response: {final_response}\n")
            
            # Update sample state
            sample['removed_passages'].append(final_response)
            del sample['candidates'][final_response]
            
            # Check if ground truth was found
            if final_response == sample['gt']:
                rank = sample['original_length'] - len(sample['removed_passages']) + 1
                sample['ground_truth_rank'] = rank
                rank_findings[rank] = rank_findings.get(rank, 0) + 1
        
        # Check if all samples have reached final state
        all_complete = True
        for sample in active_samples:
            if len(sample['candidates']) > 1:
                all_complete = False
                break
        
        if all_complete:
            # Process final state for all samples
            for sample in active_samples:
                if len(sample['candidates']) == 1:
                    if sample['ground_truth_rank'] is None:
                        sample['ground_truth_rank'] = 1
                        rank_findings[1] = rank_findings.get(1, 0) + 1
                
                # Extract query from the problem
                query = sample['problem'].split("and candidate passages")[0].split("Here is a query:")[1].strip()
                
                results.append({
                    'query': query,
                    'reasoning': sample['reasoning'],
                    'ground_truth_rank': sample['ground_truth_rank'],
                    'removed_passages': sample['removed_passages'],
                    'final_candidates': list(sample['candidates'].keys()),
                    'iterations': iteration,
                    'final_remaining': len(sample['candidates'])
                })
            break
    
    # Print summary of ground truth findings by rank
    print("\nGround Truth Findings Summary by Rank:")
    for rank in sorted(rank_findings.keys()):
        print(f"Rank {rank}: Found {rank_findings[rank]} ground truths")
    print(f"Total ground truths found: {sum(rank_findings.values())}")
    print(f"\nProcessing complete. Processed {len(results)} samples.")
    return results

def calculate_metrics(results: List[Dict[str, Any]]) -> Dict[str, float]:
    """Calculate evaluation metrics from results."""
    metrics = {
        'hit@3': [],
        'hit@5': [],
        'hit@10': [],
        'ndcg@3': [],
        'ndcg@5': [],
        'ndcg@10': [],
        'mrr': []
    }
    
    for result in results:
        ground_truth_rank = result['ground_truth_rank']
        
        # Calculate Hit@k
        for k in [3, 5, 10]:
            metrics[f'hit@{k}'].append(1 if ground_truth_rank <= k else 0)
        
        # Calculate NDCG@k
        for k in [3, 5, 10]:
            if ground_truth_rank <= k:
                dcg = 1.0 / np.log2(ground_truth_rank + 1)
                idcg = 1.0
                metrics[f'ndcg@{k}'].append(dcg / idcg)
            else:
                metrics[f'ndcg@{k}'].append(0.0)
        
        # Calculate MRR
        metrics['mrr'].append(1.0 / ground_truth_rank)
    
    # Calculate averages
    return {k: np.mean(v) for k, v in metrics.items()}

def parse_problem_text(problem: str) -> Dict[str, str]:
    """Parse the problem text to extract query and create candidate dictionary.
    
    Args:
        problem: The problem text containing query and passages
        
    Returns:
        Dict containing query and candidate passages dictionary
    """
    # Extract query
    query = problem.split("and candidate passages:")[0].replace("## Here is a query:", "").strip()
    
    # Extract passages
    passages_text = problem.split("and candidate passages:")[1].split("Please think step by step")[0].strip()
    
    # Create dictionary of passages
    passages_dict = {}
    for line in passages_text.split("\n"):
        if line.startswith("passage"):
            passage_id = line.split(":")[0].strip()
            passage_text = line.split(":", 1)[1].strip()
            passages_dict[passage_id] = passage_text
    
    return {
        "query": query,
        "candidates": passages_dict
    }

def main(dataset_name: str, gpu_id: str, model_path: str):
    if dataset_name not in CONFIG:
        raise ValueError(f"Unknown dataset: {dataset_name}. Available datasets: {list(CONFIG.keys())}")
    
    config = CONFIG[dataset_name].copy()  # Create a copy to avoid modifying the original
    config['checkpoint_path'] = model_path  # Override the checkpoint path
    
    # Set GPU device
    os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
    
    # Initialize GPU
    if not torch.cuda.is_available():
        raise RuntimeError("No CUDA device available!")
    
    device = torch.device("cuda")
    print(f"Using device: {device}")
    
    # Load model
    print("Loading model...")
    llm = LLM(
        model=config['checkpoint_path'],
        gpu_memory_utilization=0.95
    )
    
    # Configure sampling parameters
    sampling_params = SamplingParams(
        temperature=0,
        top_p=0.7,
        max_tokens=1024,
        stop=["</s>", "<|endoftext|>"],
    )
    
    # Load dataset from HuggingFace
    print("Loading dataset from HuggingFace...")
    ds = load_dataset("ulab-ai/Ranking-bench", "direct")['test']
    
    # Filter dataset by task name
    ds = ds.filter(lambda x: x['task_name'] == dataset_name)
    
    # Convert dataset to list of dictionaries with required format
    data = []
    for item in ds:
        parsed = parse_problem_text(item['problem'])
        data.append({
            'problem': item['problem'],
            'gt': item['gt'],
            'candidates': parsed['candidates']
        })
    
    # Process all data in one go
    results = process_batch(data, llm, sampling_params, dataset_name)
    
    # Calculate and save metrics
    metrics = calculate_metrics(results)
    print("\nEvaluation Results:")
    for metric, score in metrics.items():
        print(f"{metric}: {score:.4f}")
    
    output_file = f"./eval/eval_result/{dataset_name}_{config['checkpoint_path'].split('/')[-1]}.json"
    print(f"\nSaving results to {output_file}...")
    # Create directory if it doesn't exist
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # Save both metrics and detailed results
    output_data = {
        'metrics': metrics,
        'detailed_results': results
    }
    
    with open(output_file, 'w') as f:
        json.dump(output_data, f, indent=4)
    print("Done!")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='Evaluate passage ranking model on MSMARCO dataset')
    parser.add_argument('--dataset', type=str, default='Passage-5',
                      help='Dataset to evaluate on (default: Passage-5)')
    parser.add_argument('--gpu_id', type=str, default='0',
                      help='GPU ID to use (default: 0)')
    parser.add_argument('--model_path', type=str, required=True,
                      help='Path to the model checkpoint to use for evaluation')
    args = parser.parse_args()
    main(args.dataset, args.gpu_id, args.model_path) 